# FinBERT-VCSenti: MÃ´ HÃ¬nh PhÃ¢n TÃ­ch Quan Äiá»ƒm Tin Tá»©c TÃ i ChÃ­nh

## ğŸ“ Tá»•ng Quan Dá»± Ãn

**FinBERT-VCSenti** lÃ  má»™t mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Æ°á»£c fine-tune tá»« `bert-base-uncased` Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  phÃ¢n loáº¡i quan Ä‘iá»ƒm trong cÃ¡c vÄƒn báº£n tÃ i chÃ­nh báº±ng tiáº¿ng Anh. MÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng xÃ¡c Ä‘á»‹nh xem má»™t cÃ¢u mang sáº¯c thÃ¡i **TÃ­ch cá»±c (positive)**, **TiÃªu cá»±c (negative)**, hay **Trung láº­p (neutral)**, há»— trá»£ cÃ¡c bÃ i toÃ¡n tá»± Ä‘á»™ng hÃ³a trong ngÃ nh tÃ i chÃ­nh - ngÃ¢n hÃ ng.

[cite_start]Dá»± Ã¡n nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° má»™t minh chá»©ng vá» ká»¹ nÄƒng á»©ng dá»¥ng cÃ¡c mÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) vÃ o giáº£i quyáº¿t bÃ i toÃ¡n thá»±c táº¿, láº¥y cáº£m há»©ng tá»« paper nghiÃªn cá»©u **FinBERT**[Link Paper](https://arxiv.org/pdf/1908.10063)


---

## ğŸš€ MÃ´ Táº£ MÃ´ HÃ¬nh

* **Model gá»‘c (Base Model):** `bert-base-uncased` tá»« Hugging Face.
* [cite_start]**Dataset:** MÃ´ hÃ¬nh Ä‘Æ°á»£c fine-tune trÃªn bá»™ dá»¯ liá»‡u **Financial PhraseBank** [cite: 199][cite_start], cá»¥ thá»ƒ lÃ  táº­p `sentences_allagree` nÆ¡i táº¥t cáº£ cÃ¡c chuyÃªn gia tÃ i chÃ­nh Ä‘á»u Ä‘á»“ng thuáº­n vá» nhÃ£n quan Ä‘iá»ƒm[cite: 201].
* **BÃ i toÃ¡n:** PhÃ¢n loáº¡i vÄƒn báº£n (Text Classification) vá»›i 3 nhÃ£n: `positive`, `negative`, `neutral`.

---

## ğŸ› ï¸ CÃ i Äáº·t

Äá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh nÃ y, báº¡n cáº§n cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:

```bash
pip install transformers torch
```

---

## ğŸ’¡ CÃ¡ch Sá»­ Dá»¥ng

Báº¡n cÃ³ thá»ƒ dá»… dÃ ng sá»­ dá»¥ng mÃ´ hÃ¬nh nÃ y thÃ´ng qua `pipeline` cá»§a thÆ° viá»‡n Transformers.

```python
from transformers import pipeline
import torch

# Thay 'your-huggingface-username/FinBERT-VCSenti' báº±ng Ä‘Æ°á»ng dáº«n model cá»§a báº¡n sau khi push lÃªn Hub
model_checkpoint = "your-huggingface-username/FinBERT-VCSenti"
device = 0 if torch.cuda.is_available() else -1

# Khá»Ÿi táº¡o pipeline
classifier = pipeline("text-classification", model=model_checkpoint, device=device)

# Chuáº©n bá»‹ cÃ¢u cáº§n dá»± Ä‘oÃ¡n
sentences = [
    "The company's revenue grew by 25% in the last quarter.",
    "There are concerns about the upcoming economic recession.",
    "The new CEO will start his position next Monday."
]

# Thá»±c hiá»‡n dá»± Ä‘oÃ¡n
results = classifier(sentences)
for sentence, result in zip(sentences, results):
    print(f"'{sentence}'")
    print(f" -> Dá»± Ä‘oÃ¡n: {result['label'].upper()} (Score: {result['score']:.4f})\n")

```
**Káº¿t quáº£ dá»± kiáº¿n:**
```
'The company's revenue grew by 25% in the last quarter.'
 -> Dá»± Ä‘oÃ¡n: POSITIVE (Score: 0.9987)

'There are concerns about the upcoming economic recession.'
 -> Dá»± Ä‘oÃ¡n: NEGATIVE (Score: 0.9995)

'The new CEO will start his position next Monday.'
 -> Dá»± Ä‘oÃ¡n: NEUTRAL (Score: 0.9998)
```
---

## âš™ï¸ Quy TrÃ¬nh Huáº¥n Luyá»‡n

MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng `Trainer` API tá»« thÆ° viá»‡n Transformers. [cite_start]CÃ¡c siÃªu tham sá»‘ (hyperparameters) chÃ­nh Ä‘Æ°á»£c lá»±a chá»n dá»±a trÃªn Ä‘á» xuáº¥t tá»« paper FinBERT[cite: 14]:

* [cite_start]**Learning Rate:** `2e-5` [cite: 316]
* **Batch Size:** `16`
* **Sá»‘ Epochs:** `4`
* **Weight Decay:** `0.01`
* [cite_start]**Warmup Proportion:** `0.2` [cite: 316]

---

## ğŸ“Š Káº¿t Quáº£ ÄÃ¡nh GiÃ¡

MÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u nÄƒng áº¥n tÆ°á»£ng trÃªn táº­p dá»¯ liá»‡u kiá»ƒm thá»­ (validation set):

* **Accuracy:** `[Äiá»n Ä‘á»™ chÃ­nh xÃ¡c báº¡n Ä‘áº¡t Ä‘Æ°á»£c, vÃ­ dá»¥: 0.8627]`
* **F1-Score (Weighted):** `[Äiá»n F1-score báº¡n Ä‘áº¡t Ä‘Æ°á»£c, vÃ­ dá»¥: 0.9044]`

Káº¿t quáº£ nÃ y cho tháº¥y mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i tá»‘t vÃ  phÃ¹ há»£p Ä‘á»ƒ triá»ƒn khai vÃ o cÃ¡c á»©ng dá»¥ng thá»±c táº¿.

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

* Araci, D. (2019). *FinBERT: Financial Sentiment Analysis with Pre-trained Language Models*. [arXiv:1908.10063](https://arxiv.org/abs/1908.10063).
* Malo, P., Sinha, A., Korhonen, P., Wallenius, J., & Takala, P. (2014). *Good debt or bad debt: Detecting semantic orientations in economic texts*. Journal of the Association for Information Science and Technology, 65(4), 782-796.
